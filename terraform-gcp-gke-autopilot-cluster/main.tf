terraform {
  required_version = ">=1.3"

  required_providers {
    google = {
      source  = "hashicorp/google"
      version = ">= 5.25.0, < 6"
    }

    google-beta = {
      source  = "hashicorp/google-beta"
      version = ">= 5.25.0, < 6"
    }

    null = {
      source  = "hashicorp/null"
      version = "> 3.0"
    }

    time = {
      source  = "hashicorp/time"
      version = "> 0.10"
    }

    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = ">= 2.30.0, < 3"
    }

    random = {
      source  = "hashicorp/random"
      version = ">= 3.6, < 4"
    }

    tls = {
      source  = "hashicorp/tls"
      version = ">= 4.0, < 5"
    }
  }
}

data "google_client_config" "default" {}

data "google_dns_managed_zone" "dns_zone" {
  name = var.dns_zone_name
}

resource "random_id" "random_role_id_suffix" {
  byte_length = 2
}

# placed this here because it conflicts with terragrunt autogenerated files
provider "kubernetes" {
  host                   = "https://${google_container_cluster.default.endpoint}"
  token                  = data.google_client_config.default.access_token
  cluster_ca_certificate = base64decode(google_container_cluster.default.master_auth[0].cluster_ca_certificate)
}

provider "helm" {
  kubernetes {
    host                   = "https://${google_container_cluster.default.endpoint}"
    token                  = data.google_client_config.default.access_token
    cluster_ca_certificate = base64decode(google_container_cluster.default.master_auth[0].cluster_ca_certificate)
  }
}

resource "google_compute_network" "default" {
  #checkov:skip=CKV2_GCP_18:default firewall is okay for this use case
  name = "${local.gke_cluster_name}-network"

  auto_create_subnetworks = "false"
  project                 = var.project_id

  # everything in this solution is deployed regionally
  routing_mode = "REGIONAL"
}

# this will construct a vpc-native, private gke cluster. for effective routing from the regional external http load balancer,
# https://cloud.google.com/kubernetes-engine/docs/how-to/standalone-neg
resource "google_compute_subnetwork" "default" {
  #checkov:skip=CKV_GCP_26:vpc flow logs are not necessary in this context
  #checkov:skip=CKV_GCP_76:todo: check if this is a bug
  ip_cidr_range = local.internal_subnet_cidr
  name          = "${local.gke_cluster_name}-subnet"
  project       = google_compute_network.default.project
  region        = var.region
  network       = google_compute_network.default.name

  private_ip_google_access   = true
  private_ipv6_google_access = true

  depends_on = [
    google_compute_network.default
  ]
}

resource "google_compute_subnetwork" "proxy" {
  #checkov:skip=CKV_GCP_76:private access is enabled
  #checkov:skip=CKV_GCP_74:not relevant in proxy-only subnet
  #checkov:skip=CKV_GCP_26:VPC flow logs are not necessary in this context

  provider = google-beta
  name     = "${var.project_id}-proxy-only-subnet"

  ip_cidr_range = local.proxy_only_ipv4_cidr
  project       = google_compute_network.default.project
  network       = google_compute_network.default.id
  region        = var.region

  purpose = "REGIONAL_MANAGED_PROXY"
  #purpose = "GLOBAL_MANAGED_PROXY"
  role = "ACTIVE"

  depends_on = [
    google_compute_network.default
  ]
}

resource "google_compute_address" "default" {
  name         = "${local.gke_cluster_name}-ip-address"
  project      = google_compute_subnetwork.default.project
  network_tier = "STANDARD"
}

resource "google_container_cluster" "default" {
  #checkov:skip=CKV_GCP_12:nework policy is active
  #checkov:skip=CKV_GCP_18:public access is necessary for this setup
  provider           = google-beta
  project            = var.project_id
  name               = local.gke_cluster_name
  location           = var.region
  initial_node_count = var.num_nodes

  # enable cilium. if you want to use calico, enter
  # LEGACY_DATAPATH instead
  datapath_provider = "ADVANCED_DATAPATH"

  networking_mode = "VPC_NATIVE"
  network         = google_compute_network.default.name
  subnetwork      = google_compute_subnetwork.default.name

  # allow net admin capabilities to spawn wireguard endpoints
  allow_net_admin = true

  # provision an autopilot cluster to make it free tier
  # (applicable only once per billing account)
  enable_autopilot = true

  node_pool_auto_config {
    network_tags {
      tags = [local.gke_cluster_name]
    }
  }

  logging_config {
    enable_components = [
      "SYSTEM_COMPONENTS"
    ]
  }

  release_channel {
    channel = var.release_channel
  }

  workload_identity_config {
    workload_pool = "${var.project_id}.svc.id.goog"
  }

  addons_config {
    # enable GCS backed volumes
    gcs_fuse_csi_driver_config {
      enabled = true
    }

    http_load_balancing {
      # this needs to be enabled for the neg to be automatically created for the ingress gateway svc
      disabled = false
    }
  }

  private_cluster_config {
    # need to use private nodes for vpc-native gke clusters
    enable_private_nodes = true
    # allow private cluster master to be accessible outside of the network
    enable_private_endpoint = false
    master_ipv4_cidr_block  = local.master_ipv4_cidr_block
  }

  ip_allocation_policy {
    cluster_ipv4_cidr_block  = local.cluster_ipv4_cidr_block
    services_ipv4_cidr_block = local.services_ipv4_cidr_block
  }

  default_snat_status {
    # more info on why snat needs to be disabled: https://cloud.google.com/kubernetes-engine/docs/how-to/alias-ips#enable_pupis
    # this applies to vpc-native gke clusters
    disabled = true
  }

  master_authorized_networks_config {
    cidr_blocks {
      # Because this is a private cluster, need to open access to the Master nodes in order to connect with kubectl
      cidr_block   = "0.0.0.0/0"
      display_name = "World"
    }
  }

  # allow cluster deletion
  deletion_protection = false
}

resource "time_sleep" "wait_for_kube" {
  depends_on = [google_container_cluster.default]
  # GKE master endpoint may not be immediately accessible, resulting in error, waiting does the trick
  create_duration = "30s"
}

resource "null_resource" "local_k8s_context" {
  depends_on = [time_sleep.wait_for_kube]
  provisioner "local-exec" {
    # update your local gcloud and kubectl credentials for the newly created cluster
    command = "for i in 1 2 3 4 5; do gcloud container clusters get-credentials ${local.gke_cluster_name} --project=${var.project_id} --region=${var.region} && break || sleep 60; done"
  }
}

resource "google_compute_firewall" "default" {
  count = var.proxy_mode == "TCP" ? 1 : 0

  name    = "${local.gke_cluster_name}-fw-allow-health-check-and-proxy"
  network = google_compute_network.default.id
  project = google_compute_network.default.project

  # allow for ingress from the health checks and the managed envoy proxy. for more information, see:
  # https://cloud.google.com/load-balancing/docs/https#target-proxies
  source_ranges = ["130.211.0.0/22", "35.191.0.0/16", local.proxy_only_ipv4_cidr]

  allow {
    protocol = "tcp"
  }

  target_tags = [
    local.gke_cluster_name
  ]

  direction = "INGRESS"
}

